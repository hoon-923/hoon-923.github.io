---
layout: post
title:  "밑바닥부터 시작하는 딥러닝3 - STEP5"
date:   2020-12-18 10:48:15
author: Hoon
categories: 딥러닝
use_math: true
---

----

###  연쇄 법칙

딥러닝에서 가장 중요한 개념 중 하나인 역전파(backpropagation)을 이해하기 위해서는 [연쇄 법칙(chain rule)](https://ko.wikipedia.org/wiki/%EC%97%B0%EC%87%84_%EB%B2%95%EC%B9%99)이 무엇인지 알아야 한다. 연쇄 법칙에 따르면 합성 함수의 미분은 구성 함수 각각을 미분한 후 곱한 것과 동일하다. 

![5-1.PNG](https://github.com/hoon-923/hoon-923.github.io/blob/main/_images/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%EB%94%A5%EB%9F%AC%EB%8B%9D3/STEP_5/5-1.PNG?raw=true)

위의 계산 그래프를 보면 $y=F(x)$라는 함수는 $a=A(x)$, $b=B(a)$, $y=C(b)$라는 세 함수로 구성되어 있다. 이때 x에 대한 y의 미분은 $dx/dy = dy/dy * dy/db * db/da * da/dx$로 표현할 수 있다.

**<span style="color:red">NOTE</span>**  $dy/dy$는 $y$의 $y$ 미분이라 항상 1이지만 추후 역전파 구현 시 이해를 돕기 위해 포함

----

###  역전파 원리 도출

위의 식 $dx/dy = dy/dy * dy/db * db/da * da/dx$ 에서 역전파는 출력에서 입력 방향으로 미분을 계산한다. 이를 계산 그래프로 나타내면 다음과 같다.

![5-2.PNG](https://github.com/hoon-923/hoon-923.github.io/blob/main/_images/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%EB%94%A5%EB%9F%AC%EB%8B%9D3/STEP_5/5-2.PNG?raw=true)

여기서 알 수 있는 사실은 주황원안에 있는 값들은 모두 y의 대한 미분값들 이라는 것이다. 즉 변수 y, b, a, x의 미분값이 출력쪽에서 입력쪽으로 전파되는 과정이고 이것이 바로 역전파의 구체적인 과정이다. 전파되는 데이터들은 모두 y의 미분값들이다. 

-----

###  계산 그래프로 살펴보기

다음 계산 그래프를 보면 순전파와 역전파의 관계를 명확하게 볼 수 있다.

![5-5.PNG](https://github.com/hoon-923/hoon-923.github.io/blob/main/_images/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%EB%94%A5%EB%9F%AC%EB%8B%9D3/STEP_5/5-5.PNG?raw=true)

-----

**출처:\- 사이토 고키, **『**밑바닥부터 시작하는 딥러닝3**』, 개앞맵시, 한빛미디어(2020)