---
layout: post
title:  "밑바닥부터 시작하는 딥러닝3 - STEP 18"
date:   2021-01-17 12:15:15
author: Hoon
categories: 딥러닝
---

----

#### 필요 없는 미분값 삭제

기존의 DeZero에서는 모든 변수가 미분값을 변수에 저장해두고 있다.

~~~python
x0 = Variable(np.array(1.0))
x1 = Variable(np.array(1.0))
t = add(x0, x1)
y = add(x0, t)
y.backward()

print(y.grad, t.grad)
print(x0.grad, x1.grad)
~~~

실행 결과

~~~python
1.0 1.0
2.0 1.0
~~~

대부분의 경우 역전파로 구하고 싶은 미분값은 `x0`, `x1` 둘뿐일 것이지만 기존 코드의 경우 모든 변수가 미분 결과를 메모리에 유지한다. 위의 예시에서는 `y`와 `t` 같은 중간 변수의 미분값은 필요하지 않으므로 이를 고려한 코드는 다음과 같다.

~~~python
class Variable:
    ....
    
    def backward(self, retain_grad=False):
        if self.grad is None:
            self.grad = np.ones_like(self.data)

        funcs = []
        seen_set = set()

        def add_func(f):
            if f not in seen_set:
                funcs.append(f)
                seen_set.add(f)
                funcs.sort(key=lambda x: x.generation)

        add_func(self.creator)

        while funcs:
            f = funcs.pop()
            gys = [output().grad for output in f.outputs]  # output is weakref
            gxs = f.backward(*gys)
            if not isinstance(gxs, tuple):
                gxs = (gxs,)

            for x, gx in zip(f.inputs, gxs):
                if x.grad is None:
                    x.grad = gx
                else:
                    x.grad = x.grad + gx

                if x.creator is not None:
                    add_func(x.creator)

            if not retain_grad:
                for y in f.outputs:
                    y.grad = None
~~~

`retain_grad` 를 메서드 인자로 추가함으로 말단 변수 외에는 미분값을 유지하지 않도록 한다. 

~~~python
x0 = Variable(np.array(1.0))
x1 = Variable(np.array(1.0))
t = add(x0, x1)
y = add(x0, t)
y.backward()

print(y.grad, t.grad)
print(x0.grad, x1.grad)
~~~

실행 결과

~~~python
None None
2.0 1.0
~~~

이제 중간 변수인 `y` 와 `t` 의 미분값이 삭제되어 그만큼의 메모리를 다른 용도로 사용할 수 있게 되었다.

----

#### Function 클래스 복습

DeZero에서 미분을 하러면 순전파를 수행한 뒤 역전파를 해주어야 하고 이를 위해 역전파 시 필요한 순전파의 계산 결괏값을 저장해둔다.

~~~python
class Function:
  def __call__(self, *inputs):
    xs = [x.data for x in inputs]
    ys = self.forward(*xs)
    if not isinstance(ys, tuple): # 튜플이 아닌 경우 추가 지원
      ys = (ys,)	
    outputs = [Variable(as_array(y)) for y in ys]

    self.generation = max([x.generation for x in inputs])
    for output in outputs:
      output.set_creator(self)
    self.inputs = inputs
    self.outputs = [weakref.ref(output) for output in outputs]

    return outputs if len(outputs) > 1 else outputs[0]
~~~

위 코드에서 `self.inputs = inputs` 부분이 결괏값을 저장해두는 역할을 한다. 입력을 `inputs` 라는 인스턴스 변수로 참조하는 변수의 카운트가 1만큼 증가하고, `__call__` 메서드를 벗어난 뒤에도 메모리에 남게 된다.

역전파하는 경우라면 참조할 변수들을 `inputs` 에 미리 보관해둬야 하지만 떄로 미분값이 필요 없는 경우도 있다. 이 경우 중간 계산 결과를 저장할 필요가 없다.

----

#### Config 클래스를 활용한 모드 전환

순전파만 할 경우를 위한 개선을 추가하기 위해 역전파 활성 모드와 역전파 비활성 모드를 전환하는 구조가 필요하다.

~~~python
class Config:
  enable_backprop = True
~~~

불리언 타입인 `enable_backprop` 을 이용해 역전파 가능 여부를 결정하였다. 설정 데이터는 한곳에만 존재하는 것이 좋기 때문에 인스턴스화하지 않고 클래스 상태로 이용하도록 하였다.

~~~python
class Function:
  def __call__(self, *inputs):
    xs = [x.data for x in inputs]
    ys = self.forward(*xs)
    if not isinstance(ys, tuple):
      ys = (ys,)	
    outputs = [Variable(as_array(y)) for y in ys]

    if Config.enable_backprop:
      self.generation = max([x.generation for x in inputs])
      for output in outputs:
        output.set_creator(self)
      self.inputs = inputs
      self.outputs = [weakref.ref(output) for output in outputs]

    return outputs if len(outputs) > 1 else outputs[0]
~~~

`Config.enable_backprop` 이 `True` 인 경우에만 역전파 코드가 실행되도록 기존 코드를 수정하였다.

----

#### 모드 전환

~~~python
Config.enable_backprop = True
x = Variable(np.ones((100, 100, 100)))
y = square(square(square(x)))
y.backward()

Config.enable_backprop = False
x = Variable(np.ones((100, 100, 100)))
y = square(square(square(x)))
~~~

크기가 큰 다차원 배열인 형상이 (100, 100, 100)인 텐서를 이용해 실습하였다. `square` 함수를 세 번 적용(원소별 제곱)하면 `Config.enable_backprop = True` 인 경우는 중간 계산 결과가 계속 유지되어 그만큼 메모리를 차지하게 된다. 반면 `Config.enable_backprop = False` 인 경우 중간 계산 결과는 사용 직후 메모리에서 삭제 된다.

----

#### with 문을 활용한 모드 전환

파이썬에서는 후처리를 자동으로 수행하고자 할 때 `with`  구문을 사용할 수 있다. 만약 이를 사용하지 않고 파일에 무언가를 작성하려면 다음과 같이 `open` 과 `close` 를 이용해야 한다.

~~~python
f = open('sample.txt', 'w')
f.write('hellow world!')
f.close()
~~~

간혹 실수로 `close()` 를 까먹을수도 있고 매번 사용하기도 귀찮을수 있다. 

하지만 `with` 를 사용하면 다음과 같이 코드를 수정할 수 있다.

~~~python
with open('sample.txt', 'w') as f:
  f.write('hello world!')
~~~

위의 코드에서는 `with` 블록에 들어갈 때 파일이 열리고 빠져나옴과 동시에 자동으로 닫힌다. 이와 같은 원리를 이용해 역전파 비활성 모드 전환을 수정하려고 한다.

~~~python
with using_config('enable_backprop', False):
  x = Variable(np.array(2.0))
  y = Square(x)
~~~

아직 `using_config` 함수를 구현한 것이 아니기 때문에 위의 코드가 정상적으로 실행 되지는 않는다. 

예시로 `contextlib` 모듈을 이용해 `with` 문을 사용한 모드 전환을 구현하면 다음과 같다.

~~~python
import contextlib

@contextlib.contextmanager
def config_test():
  print('start') # 전처리
  try:
    yield
  finally:
    print('done') # 후처리

with config_test():
  print('process')
~~~

실행 결과

~~~python
start
process
done
~~~

`@contextlib.contextmanager` 데코레이터를 달면 문맥을 판단하는 함수가 만들어진다. 이 함수 내부에서 `yield` 전에는 전처리 로직을, `yield` 다음에는 후처리 로직을 작성한다. 후에 `with` 를 이용하면 블록안으로 들어갈 때 전처리가 실행되고 빠져나올 때 자동으로 후처리가 실행된다.

이를 이용해 `using_config` 를 다음과 같이 구현할 수 있다.

~~~python
import contextlib

@contextlib.contextmanager
def using_config(name, value):
  old_value = getattr(Config, name)
  setattr(Config, name, value)
  try:
    yield
  finally:
    setattr(Config, name, old_value)
~~~

~~~python
with using_config('enable_backprop', False):
  x = Variable(np.array(2.0))
  y = square(x)
~~~

이와 같이 역전파가 필요없는 경우에는 `with` 블록을 이용해 순전파 코드만 실행하도록 하였다. 매번 `with using_config('enable_backprop', False):` 를  작성하기 귀찮기 때문에 코드를 간소화 하기 위해 마지막으로 다음과 같은 수정을 하였다.

~~~python
def no_grad():
  return using_config('enable_backprop', False):
~~~

----

**출처:\- 사이토 고키, **『**밑바닥부터 시작하는 딥러닝3**』, 개앞맵시, 한빛미디어(2020)