---

layout: post
title:  "밑바닥부터 시작하는 딥러닝3 - STEP 7"
date:   2020-12-19 14:54:15
author: Hoon
categories: 딥러닝
---

-----

###  역전파 자동화의 시작

이전 단계에서 역전파를 구현했지만 매 계산 과정마다 수동으로 구현해야 한다는 불편함이 있었다. 이번 단계에서는 이러한 역전파 과정을 자동화 하려고 한다.  이를 Define-by-Run 이라고 하는데 이는 딥러닝에서 수행하는 계산들을 계산 시점에 연결하는 방식이다. 즉 계산 그래프는 노드(node)와 엣지(edge)로 구성되어 있는데 평상시에는 노드들이 독립적으로 존재하다가 계산하는 시점에 엣지들로 인해 연결되는 구조를 지닌다. 

이를 코드로 구현하기 전에 우선 함수와 변수의 관계를 이해해야 한다. 

![7-2.PNG](https://github.com/hoon-923/hoon-923.github.io/blob/main/_images/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%EB%94%A5%EB%9F%AC%EB%8B%9D3/STEP_7/7-2.PNG?raw=true)

위의 계산 그래프 그림에서 왼쪽은 함수 입장에서 변수는 입력 변수(input)와 출력 변수(output)으로 존재함을 보여준다. 마찬가지로 오른쪽 그림은 변수 입장에서 함수는 창조자(creator)이다. 이와 같은 둘의 관계를 고려하기 위해 기존  코드를 다음과 같이 수정한다.

~~~python
class Variable:
  def __init__(self, data):
    self.data = data
    self.grad = None
    self.creator = None # creator라는 인스턴스 변수 추가
  
  def set_creator(self, func):
    self.creator = func
~~~

~~~python
class Function:
  def __call__(self, input):
    x = input.data
    y = self.forward(x)
    output = Variable(y)
    output.set_creator(self) # 출력 변수에 창조자를 설정
    self.input = input
    self.output = output # 출력도 저장
    return output

  def forward(self, x):
    raise NotImplementedError()
  
  def backward(self, gy):
    raise NotImplementedError()
~~~

`Variable` 클래스에 `creator` 인스턴스 변수를 추가한 후 이를 설정하기 위해 `set_creator` 메서드를 추가하였다. 그 후 `Function` 클래스의  생성된 `output` 에 `set_creator` 를 이용해서 창조자임을 기억 시킨다. 이와 같이 변수(Variable)과 함수(Function)을 연결하면 계산 그래프를 거꾸로 올라가는 역전파를 구현할 수 있다.

~~~python
A = Square()
B = Exp()
C = Square()

x = Variable(np.array(0.5))

a = A(x)
b = B(a)
y = C(b)

# 계산 그래프의 노드들을 거꾸로 거슬러 올라간다.
assert y.creator == C # 변수 y의 creator는 C
assert y.creator.input == b
assert y.creator.input.creator == B
assert y.creator.input.creator.input == a
assert y.creator.input.creator.input.creator == A
assert y.creator.input.creator.input.creator.input == x
~~~

여기서 `assert` 문은 뒤에 오는 내용이 참이 아니면 예외를 발생시키므로 조건을 충족하는지 여부를 확인하는 데에 사용할 수 있다. 

위의 과정을 계산 그래프로 나타내면 다음과 같다.

![7-3.PNG](https://github.com/hoon-923/hoon-923.github.io/blob/main/_images/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%EB%94%A5%EB%9F%AC%EB%8B%9D3/STEP_7/7-3.PNG?raw=true)

함수와 변수 사이의 연결로 구성되고, 이 연결은 실제로 계산이 수행되는 시점에 만들어진다(Define-by-Run).

----

### 역전파 도전!

위의 역전파 과정 계산 그래프에서 y -> b -> a -> x로의 역전파를 코드로 구현하였다.

~~~python
# y에서 b까지의 역전파
y.grad = np.array(1.0)

C = y.creator # 1. 함수를 가져온다.
b = C.input # 2. 함수의 입력을 가져온다.
b.grad = C.backward(y.grad) # 3. 함수의 backward 메서드를 호출한다.

# b에서 a까지의 역전파
B = b.creator # 1. 함수를 가져온다.
a = B.input # 2. 함수의 입력을 가져온다.
a.grad = B.backward(b.grad) # 3. 함수의 backward 메서드를 호출한다.

# a에서 x까지의 역전파
A = a.creator # 1. 함수를 가져온다.
x = A.input # 2. 함수의 입력을 가져온다.
x.grad = A.backward(a.grad) # 3. 함수의 backward 메서드를 호출한다.
print(x.grad)
~~~

실행 결과

~~~
3.297442541400256
~~~

-----

### backward 메서드 추가

위의 코드에서 역전파마다 변수와 함수만 바뀐 상태로 똑같은 코드가 중복되어 나타내기 때문에 이를 자동화할 필요성이 존재한다.

~~~python
class Variable:
  def __init__(self, data):
    self.data = data
    self.grad = None
    self.creator = None
  
  def set_creator(self, func):
    self.creator = func

  def backward(self):
    f = self.creator # 1. 함수를 가져온다
    if f is not None:
      x = f.input # 2. 함수의 입력을 가져온다
      x.grad = f.backward(self.grad) # 3. 함수의 backward 메서드를 호출한다.
      x.backward()  # 하나 앞 변수의 backward 메서드를 호출한다(재귀)
~~~

위의 `Variable`  인스턴스에서 `creator` 가 `None` 이면 역전파가 중단된다. 

~~~python
A = Square()
B = Exp()
C = Square()

x = Variable(np.array(0.5))

a = A(x)
b = B(a)
y = C(b)

#역전파

y.grad = np.array(1.0)
y.backward()
print(x.grad)
~~~

실행 결과

~~~
3.297442541400256
~~~

----



**출처:\- 사이토 고키, **『**밑바닥부터 시작하는 딥러닝3**』, 개앞맵시, 한빛미디어(2020)

